{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "Cuando necesitamos extraer información publicada en internet, lo ideal es consultar una API, porque:\n",
    "\n",
    "* Las respuestas contienen información estructurada\n",
    "* En general, el propio servicio nos da documentación sobre cómo hacer peticiones y qué tipo de información podemos solicitar\n",
    "\n",
    "Pero muchas veces nos encontramos con información en páginas web (en formato [HTML](https://es.wikipedia.org/wiki/HTML)) que nos gustaría obtener, pero sin API disponible.\n",
    "\n",
    "Estas páginas `HTML` tienen cierta estructura, aunque con ciertos contras:\n",
    "\n",
    "* Es más compleja, puede tener muchos niveles de anidamiento\n",
    "* Es inestable. Están diseñadas para que se vean bien desde el explorador, no para guardar una estructura de consulta. De un día para otro, puede verse alterada por la incorporación de nuevos elementos visuales u otros motivos.\n",
    "* Puede ser modificada por código cliente (javascript) en diferentes momentos: al cargar la página, al interaccionar con algún elemento, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Desde tu explorador, consulta el código fuente de una página de tu interés. Por ejemplo, para hacerlo en chrome:\n",
    "\n",
    "* Accede a la página, p.e. [esta](https://es.wikipedia.org/wiki/HTML).\n",
    "* Haz click derecho y pulsa `View page source`. Otra opción es pulsar `Inspect`, que además abrirá las herramientas de desarrollador de Chrome, muy útiles para navegar por la estructura de la página."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping de elementos html\n",
    "\n",
    "La librería que vamos a utilizar es [Beautiful Soup](https://pypi.org/project/beautifulsoup4/). Nos permite buscar elementos y navegar por la estructura del html fácilmente.\n",
    "\n",
    "Vemos dos ejemplos, uno sobre milanuncios y otro sobre spotahome, por si nos _banean_.\n",
    "\n",
    "### milanuncios\n",
    "\n",
    "Imaginemos que queremos comparar precios de un determinado modelo de motocicleta de segunda mano. P.e. con [esta búsqueda](https://www.milanuncios.com/motos-de-carretera/duke-390.htm) en milanuncios.\n",
    "\n",
    "La mayor parte de las webs con contenido interesante (que hacen negocio gracias a su contenido) intentan protegerlas para evitar que les hagan scraping. Hay varias formas de simular que nuestro script es humano en lugar de un bot, algunas más básicas y otras más complejas. Por ahora, vamos a sobrescribir nuestro _user agent_. Es una cabecera que va en las peticiones diciendo quiénes somos (p.e. qué tipo de explorador usamos). Por defecto, la librería `requests` que vamos a usar, avisa que somos un bot. Vamos a sobrescribir esta cabecera para _disimular_ un poco. Podemos copiar uno popular de (aquí)[https://developers.whatismybrowser.com/useragents/explore/software_type_specific/web-browser/]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, nos descargamos el html con `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.milanuncios.com/motos-de-carretera/duke-390.htm', headers=headers)\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el contenido examinando la propiedad `content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este contenido es solo texto, no tiene estructura. Aún no podemos hacer búsquedas ni navegar por él.\n",
    "\n",
    "Para hacerlo, creamos una instancia de `Beautiful Soup` y lo parseamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags = soup('div')\n",
    "tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOS: ¡me han baneado!\n",
    "\n",
    "Si estamos haciendo esto en clase, es muy probable que a la mayoría, nos detecten como bots y nos baneen. Esto pasa porque somos 30 o 40 personas, haciendo la misma petición desde la misma IP y con el mismo user agent.\n",
    "\n",
    "Puedes usar el html offline que hay en `dat` para seguir con el ejercicio. Descomenta el siguiente código y sigue adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta esto si te han baneado\n",
    "# with open('dat/milanuncios.html') as f:\n",
    "#     soup = BeautifulSoup(f, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre esto, podemos hacer búsquedas con `find` y `find_all` (o `select_one` y `select` si prefieres utilizar [selectores css](https://en.wikipedia.org/wiki/Cascading_Style_Sheets#Selector)). Sobre nuestro ejemplo, vamos a buscar todos los precios. Examinando el código fuente, vemos que son etiquetas `div` con clase `aditem-price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_precios = soup.find_all('div', class_='aditem-price')\n",
    "div_precios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_all` devuelve una lista de elementos. Sobre ellos, podemos hacer:\n",
    "\n",
    "`children` para sacar el listado de todos los hijos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(div_precios[0].children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_text()` para sacar el texto de todos los hijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_precios[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, para sacar el listado de todos los precios podemos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[list(div_precio.children)[0] for div_precio in div_precios]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tienes más funciones útiles con pequeños ejemplos [aquí](http://akul.me/blog/2016/beautifulsoup-cheatsheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Crea un dataframe de pandas en el que cada fila sea un anuncio y tenga como columnas información que consideres relevante: precio, kilómetros, año, cilindrada, texto del anuncio, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "page = requests.get('https://www.milanuncios.com/motos-de-carretera/duke-390.htm', headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "columns = ['producto','tipo_anuncio','precio','texto','cc','ano','kms']\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for link in soup.find_all('div', class_='aditem ParticularCardTestABClass'):\n",
    "    producto = link.find('a', class_='aditem-detail-title').get_text()\n",
    "    tipo_anuncio = link.find('div', class_='x3').get_text()\n",
    "    if link.find('div', class_='aditem-price') is None:\n",
    "        precio = 0\n",
    "    else:\n",
    "        precio = link.find('div', class_='aditem-price').get_text().replace('€','')\n",
    "    texto = link.find('div', class_='tx').get_text()\n",
    "    if link.find('div', class_='cc tag-mobile') is None:\n",
    "        cc = 0\n",
    "    else:\n",
    "        cc = link.find('div', class_='cc tag-mobile').get_text().replace('cc','')\n",
    "    if link.find('div', class_='ano tag-mobile') is None:\n",
    "        ano = 0\n",
    "    else:\n",
    "        ano = link.find('div', class_='ano tag-mobile').get_text().replace('año','') #¿Alguna solución mejor que un if para esto?\n",
    "    if link.find('div', class_='kms tag-mobile') is None:\n",
    "        kms = 0\n",
    "    else:\n",
    "        kms = link.find('div', class_='kms tag-mobile').get_text().replace('kms','')\n",
    "\n",
    "    nueva_fila = {'producto':producto,'tipo_anuncio':tipo_anuncio,'precio':precio,'texto':texto,'cc':cc,'ano':ano,'kms':kms}\n",
    "    df = df.append(nueva_fila, ignore_index=True)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Modifica el código anterior para que, además de bajarse la página actual, navegue por el resto de páginas e incorpore también esos anuncios a tu dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.milanuncios.com/motos-de-carretera/duke-390.htm', headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "columns = ['producto','tipo_anuncio','precio','texto','cc','ano','kms']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "nueva_pag = 1\n",
    "while len(soup.find_all('div', class_='aditem ParticularCardTestABClass')) > 0:\n",
    "    for link in soup.find_all('div', class_='aditem ParticularCardTestABClass'):\n",
    "        producto = link.find('a', class_='aditem-detail-title').get_text()\n",
    "        tipo_anuncio = link.find('div', class_='x3').get_text()\n",
    "        if link.find('div', class_='aditem-price') is None:\n",
    "            precio = 0\n",
    "        else:\n",
    "            precio = link.find('div', class_='aditem-price').get_text().replace('€','')\n",
    "        texto = link.find('div', class_='tx').get_text()\n",
    "        if link.find('div', class_='cc tag-mobile') is None:\n",
    "            cc = 0\n",
    "        else:\n",
    "            cc = link.find('div', class_='cc tag-mobile').get_text().replace('cc','')\n",
    "        if link.find('div', class_='ano tag-mobile') is None:\n",
    "            ano = 0\n",
    "        else:\n",
    "            ano = link.find('div', class_='ano tag-mobile').get_text().replace('año','') #¿Alguna solución mejor que un if para esto?\n",
    "        if link.find('div', class_='kms tag-mobile') is None:\n",
    "            kms = 0\n",
    "        else:\n",
    "            kms = link.find('div', class_='kms tag-mobile').get_text().replace('kms','')\n",
    "\n",
    "        nueva_fila = {'producto':producto,'tipo_anuncio':tipo_anuncio,'precio':precio,'texto':texto,'cc':cc,'ano':ano,'kms':kms}\n",
    "        df = df.append(nueva_fila, ignore_index=True)\n",
    "\n",
    "    nueva_pag += 1\n",
    "    page = requests.get('https://www.milanuncios.com/motos-de-carretera/duke-390.htm?pagina='+str(nueva_pag), headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping de tablas\n",
    "\n",
    "A menudo, la información que nos interesa descargar está en tablas y nuestro objetivo es importarlas en tablas de Pandas. Esta conversión suele exigir la manipulación del texto, números y fechas contenidas en la tabla original, lo que nos obligará a repasar cómo realizar esas operaciones y aplicarlas a filas y columnas de las tablas.\n",
    "\n",
    "La estructura que suelen tener la tablas en `html` es:\n",
    "\n",
    "```\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Columna A</th>\n",
    "            <th>Columna B</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>A1</td>\n",
    "            <td>B1</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>A2</td>\n",
    "            <td>B2</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitaremos los siguientes módulos además de `requests` y `BeautifulSoup` importados anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, hacemos una petición para descargar la página de interés (que contiene las cotizaciones de las acciones del IBEX 35 en tiempo _casi_ real)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.eleconomista.es/indice/IBEX-35\"\n",
    "res = requests.get(base_url)\n",
    "contenido = res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente línea procesa el HTML de la página que hemos descargado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(contenido, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez procesado el HTML, es posible buscar elementos dentro de él. En particular, podemos buscar los elementos de tipo `table`, es decir, tablas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablas = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto `tablas` contiene todas las tablas presentes en la página. Hay que tener cuidado con dichas tablas porque muchas páginas utilizan elementos de tipo `table` para estructurar el contenido. Por eso, en algunas páginas, aunque parezca haber una única tabla, puede haber otras con una información no interesante que toca descartar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tablas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos extraer las filas de todas estas tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas = [x for tabla in tablas for x in tabla.find_all('tr')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para luego extraer los contenidos de cada fila individualmente haciendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = [[x.text for x  in linea.find_all('td')] for linea in lineas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos inspeccionar parte del objeto resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay filas que contienen la información de interés junto con otras que contienen cabeceras y otra información irrelevante. En general, la situación puede ser más complicada y se hace necesario estudiar el objeto `tablas` para seleccionar la de interés.\n",
    "\n",
    "En nuestro caso, podemos filtrar las líneas menos relevantes así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = [x for x in datos if len(x) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos crear una tabla de Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.DataFrame(datos)\n",
    "datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio\n",
    "\n",
    "Usa los elementos `th` de la primera fila de las tablas para extraer nombres para las columnas de la tabla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabecera = [[x.text for x  in linea.find_all('th')] for linea in lineas]\n",
    "cabecera = [x for x in cabecera if len(x) > 0]\n",
    "cabecera = cabecera[0]\n",
    "cabecera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio\n",
    "\n",
    "Elimina las columnas irrelevantes y cambia los nombres de las columnas por otros breves y sin caracteres extraños o que dificulten el posproceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabecera = [[x.text for x  in linea.find_all('th')] for linea in lineas]\n",
    "cabecera = [x for x in cabecera if len(x) > 0]\n",
    "cabecera = cabecera[0]\n",
    "cabecera.pop(2) #Elimino la columna que tiene el nombre en blanco\n",
    "\n",
    "cabecera = [re.sub('[()/. ]','',x) for x in cabecera]\n",
    "cabecera = [re.sub('ó','o',x) for x in cabecera]\n",
    "cabecera = [re.sub('%','pct',x) for x in cabecera]\n",
    "cabecera = [re.sub('€','Euros',x) for x in cabecera]\n",
    "cabecera[8] = 'Fecha'\n",
    "\n",
    "datos = datos.drop([2], axis = 1) #Elimino una columna en blanco\n",
    "\n",
    "datos.columns = cabecera\n",
    "\n",
    "datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio\n",
    "\n",
    "Cambia el formato de las columnas adecuadamente: convierte a numéricas, etc., las columnas que lo requieran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['VolumenEuros'] = datos['VolumenEuros'].replace(regex=r'\\.',value='')\n",
    "\n",
    "datos = datos.replace(regex=r'%',value='')\n",
    "datos = datos.replace(regex=r',',value='.')\n",
    "datos[['Precio','Varpct','VarEuros','VolumenEuros','Capitalizacion1','PER','RentDiv']] = datos[['Precio','Varpct','VarEuros','VolumenEuros','Capitalizacion1','PER','RentDiv']].astype(float)\n",
    "\n",
    "datos['Fecha'] = datos['Fecha'] + '/2020'\n",
    "\n",
    "datos['Fecha'] = pd.to_datetime(datos['Fecha'], format='%d/%m/%Y')\n",
    "\n",
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riesgos del scraping\n",
    "\n",
    "El scraping es una técnica potente pero tiene varios contras:\n",
    "\n",
    "* Implica mayor tiempo de desarrollo y mayor esfuerzo en la limpieza de datos (en comparación con otras fuentes como APIs, BDs, ...)\n",
    "* Si hay que scrapear gran cantidad de páginas, es lento\n",
    "* Los servidores objetivo de nuestro scraping pueden tener técnicas para evitarlo. Por ejemplo, bloquear la IP temporalmente o introducir delays en las respuestas si hacemos muchas peticiones en poco tiempo. Esto pasa especialmente en las grandes webs recelosas de sus datos (p.e. linkedin, amazon, ...).\n",
    "* El código de scraping escrito hoy puede no funcionar mañana, si la web destino cambia nombres, etiquetas o estructura. Si se sube a producción para lanzarlo periódicamente, hay que ser conscientes de que en algún momento fallará, y establecer mecanismos de alerta\n",
    "\n",
    "## Javascript\n",
    "\n",
    "Es posible que te encuentres con algún caso en el que no puedas descargar tal cual el html y parsearlo, principalmente por dos motivos:\n",
    "\n",
    "* La estructura de la página se genera parcial o totalmente en cliente\n",
    "* Debemos interactuar con algún elemento para mostrar la información que queremos (p.e. completar un campo de búsuqeda, hacer click en algún botón, ...)\n",
    "\n",
    "En estos casos, hay que ejecutar en un navegador local el código javascript de la página destino. Para esta tarea, puedes utilizar [Selenium]().\n",
    "\n",
    "[Aquí](https://medium.freecodecamp.org/better-web-scraping-in-python-with-selenium-beautiful-soup-and-pandas-d6390592e251) un post con un ejemplo de uso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
